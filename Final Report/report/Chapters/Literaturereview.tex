Many resources consider as an important factor while predicting cloud workloads CPU usage, Memory Usage, Network throughput, etc. it is a challenging task to predict the constant fluctuation in the resources for a short period (e.g., 10 to 30 min). Two approaches help to predict resource usage on a cloud. The first is a time-series analysis which helps to monitor continuous data and based on previous data forecast the future resource usage. Another approach is to machine learning and deep learning model that use a complex calculation based on previous data and predict the resource usage. In this paper, first, we are performing time-series analysis on the BitBrain dataset and after that apply LSTM Neural Network to predict the future resource usage \cite{kumar2018workload}.
 
\section{Time-Series Analysis}
 
In Virtual machines, the CPU is one of the common metrics that studied while analyzing the performance of the system. Dinda et. al. mentioned their article compare different time-series models (i.e., ARIMA, AR, MA, ARIMA, ARMA) based on CPU load. While comparing every time-series model they found that Auto Regression (AR) model with 16 or higher number of order efficiently forecast the 1hz of data up to 30 seconds in the future. The performance of the model is better compared to other model and provide optimal fit to data. AR model providing a very small amount of CPU and network throughput on 1hz of data and produce up-to-date forecasting for the system \cite{dinda2000host}.

 Parminder Singh et. al. introduced the TASM model which is a technocrat ARIMA and Support Vector Regression (SVR) machine used to predict workload on web applications available on the cloud. To check the performance of the system they used two weblogs on the proposed model and perform a comparative analysis of the other time series model (i.e Naive forecast Model, MA, ARIMA, ARMA, AR, SVR). The proposed model provides efficient result in both seasonal pattern and non-seasonal pattern data. While comparing all the error metrics that are applied on TASM and other time series model, the author found that TASM provide efficient results in all error metrics. When TASM applying on one of the weblogs that produce 47.76 MAE which is optimal compare to another model (i.e.Naive:70.02, ARIMA:80.36, ARIMA:66.39, SVR:66.77 MAE). TASM model significantly provides optimal result and improve the performance of the web application available on a cloud, as a results model mitigate cost and energy on a cloud \cite{singh2019tasm}.Vazquez et. al. mentioned in their paper how to apply different time series models to forecast workload on a cloud data center for resource provision. In this paper, the authors use several time series models (i.e., First-order AR, First Order MA, ARIMA, Neural Network Auto Regression Model) on Intel Netbatch logs and google cloud trace. while applying models on both datasets, they found that for a small period like 10 min forecasting models are less predictable compare to a 1-hr period time. While performing forecasting model on Intel Netbatch, AR method produces very low error 27.70\% error while moving average model produces the highest error 40.85\% MAPE during a 10-min period. While ARIMA and NNET model produce 29.72 and 29.75 MAPE respectively\cite{vazquez2015time}.

Calheiros et. al. \cite{calheiros2014workload} proposed the ARIMA model to solve the workload prediction for cloud-based Software-As-a-Service (SAAS) and how to impact the quality of service of the application.
The authors mentioned that dynamic resource provisioning affects the Quality of Service (QoS). if users access the service during peak time and because of static resource provisioning user may get delay while accessing the service in this particular situation Quality Of service is poor which affects the user experience on service. ARIMA model predicts the future workload based on the real-time traces of requests to web application server.  ARIMA model able to reach up to 91\% accuracy, as a result, produce greater efficiency in resource usage and Quality of Service.

\section{ Neural Network model}
 
For the last decades, machine learning provides extensive results in the field of future Prediction based on historical observation and complex calculations. To provide a more accurate result Neural Network is the key that computes a complex calculator on each node of the layer and passes the information to the next layers of nodes. It creates a complex network; each succession node receives information from all previous nodes and computes calculators like mimics of brain cells.

Zhu et. al. \cite{zhu2019novel} proposed a model of an Attention-based LSTM encoder-decoder network that helps to predict future workload on a cloud. The proposed model is divided into two parts, one with the LSTM encoder-decoder and another one is output layers. First input data pass to the encoder of LSTM converts all input data into context vector by extraction of contextual and sequential feature from the input data. Then decoder of the network decodes the context vector and predicts the future workload in the batch. Output layers convert information coming from the Decode layer to the final prediction value of the model. The proposed model used 64 dimensions hidden state in both encoder and decoder, after prediction, apply on Alibaba cluster trace 2018 and perform comparative analysis to check the efficiency of the model. ARIMA model generates 33.126\% MAPE error, Gated Recurrent Unit Encoder-Decoder (GRUED) model produce 24.362\% MAPE while our proposed model has 19.529\% MAPE. the proposed model produces state-of-art performance also scroll prediction method allows reducing the error which generates during long-term prediction and divide tasks into smaller tasks.

Gupta et. al. mentioned in their paper how Bi-Directional LSTM helps to improve the performance in workload prediction in cloud computing \cite{gupta2017resource}. The traditional time series model only able to save information about the latest past data. In Neural Network when found the local minima, itâ€™s looking for global minima by iterating the local minima which take a small step towards the negative function of gradient, which makes the learning process slow this is known as the vanishing gradient problem. To learn long-term dependency between the data LSTM is an efficient model that provides memory cells that help to store the dependency and is widely used in non-linear problems. However, LSTM only uses one direction to process the data which leads to an overfitting problem in some datasets. The proposed model is Bi-LSTM that uses 2 LSTM to work Parallel. Bi-LSTM eliminates the problem of overfitting using two hidden data layers that work in both directions. In the paper, the Author produce prediction and an analysis result with 10min, 20min, 30min interval steps, And perform comparative analysis to check performance with another model (i.e. ARIMA, LSTM -U (univariate), LSTM-M(multi-variate), Bi-LSTM-U (univariate)) with the proposed model, for 10 min of interval BILSTM with multivariate produces 0.0095 RSME of CPU usage prediction while other models like ARIMA produce 0.0198, LSTM-U with 0.00123, LSTM-M generates 0.00115, Bi-LSTM-U produces 0.00105.while for 20-min and
30-min both produce effective results of the 0.00184 and 0.00225 respectively. In the end, the author concludes that Bi-LSTM produces state-of-art performance compare to LSTM and Bi-LSTM-U prediction models.

Ouhame et. al. \cite{ouhame2021efficient} produced CNN-LSTM (Convolution Neural Network with LSTM) architecture to predict the resource utilization of cloud data centers. They also performed the comparative analysis of error metrics with a different hybrid model like VAR-MLP (Vector Auto Regression with Multi-Layer Perceptron), VAR-GRU (VAR with Gated Recurrent Unit), ARIMA-LSTM.  Based on comparison they found that VAR-MLP produced 0.3446, VAR-GRU generates 0.3295, ARIMA-LSTM with 0.3111, the and proposed model produced 0.3193 RMSE error on test data. In proposed model have 80\% training data and 20% testing data in Normalized form, for the CNN model they used 64 filters with 1 size of the kernel and ReLU (Rectified Linear) activation function with pooling size 2. CNN passes information to the LSTM model where LSTM uses 50 units with ReLU activation function of single layer and  ADAM optimizer to train the model. The author used the 1250 VM  BitBrain dataset for experiment and analysis \cite{shen2015statistical}.
Shen and Hong \cite{shen2020host} used a Bi-directional model to predict host load in cloud computing. They used one month trace of the google data center \cite{verma2015large} and based on CPU usage of all tasks running on the VM machine predict the future workload on the VM. Also compare performance metrics with the different models like LSTM ED(Encoder-Decoder), LSTM, AR (Auto Regression), ANN (Artificial Neural Network). The proposed Bi-LSTM used 24/64 size input layers,128 units for hidden layers, batch size with 128, global clipping norm rate with 5, 90 epochs, 10 early stopping rates, and 0.0.1 dropout rates.
Janaradhan and Barrett \cite{janardhanan2017cpu} mentioned about CPU workload prediction on cloud data centers using ARIMA model and LSTM model. ARIMA model performed on google data trace and based on value of (2,1,2) of (p,d,q) it generates error between 37.331\% to 42.881\% on different VM machines. For LSTM model, if the data is not stationary then applied first difference to stabilize the data without loose of information. Then tanh activation applied to normalized the data from -1 to 1. The output of RMSE error of LSTM model  is 0.0317 on training data and 0.0381 on testing data. After multiple change in hyperparameter of LSTM model authors found that LSTM model predict the workload with 17.566 to 23.65\% error rate. Hence for trace of  google data Authors conclude that LSTM is excellent model for predict the future CPU usage.



\begin{table}[h]
\caption{Summary of Literature Review} %title of the table
\centering % centering table

\begin{tabular}{| m{5em} | m{3cm}| m{3.5cm} |m{3cm}| m{2cm}|} % creating eight columns
\hline\hline %inserting double-line
Prediction Techniques & Dataset & Error Prediction &  Preprocessing data & References \\ 
\hline
Attention based LSTM encoder-Decoder & Alibaba and Dinda workload traces &	MAE,RMSE, MAPE &	Yes &\cite{zhu2019novel}	\\ \hline
Bi-Directional LSTM	 & Google data traces &	RMSE &	Yes	&\cite{gupta2017resource} \\ \hline
CNN-LSTM &	BitBrain dataset &	RMSE,MAE,MSE &	Yes &	\cite{shen2020host} \\ \hline
Bi-LSTM &	Google data trace &	MSSE,MSE &	Yes &	\cite{verma2015large} \\
\hline
LSTM and ARIMA &	Google data trace &	RMSE, MAPE	& Yes	&\cite{janardhanan2017cpu} \\  [1ex]

\hline % inserts single-line

\hline % inserts single-line
\end{tabular}

\label{tab:litreview}
\end{table}

